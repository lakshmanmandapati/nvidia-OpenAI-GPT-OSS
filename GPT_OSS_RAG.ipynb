{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/2lIZz9GBtZDXJSGJYURY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":57,"metadata":{"id":"bidmI6-xsXX7","executionInfo":{"status":"ok","timestamp":1754842795268,"user_tz":-330,"elapsed":3535,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"outputs":[],"source":["!pip install -q langchain_nvidia_ai_endpoints langchain-community langchain-text-splitters faiss-cpu"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import FAISS\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import chain"],"metadata":{"id":"AXEprXjqsds8","executionInfo":{"status":"ok","timestamp":1754842809858,"user_tz":-330,"elapsed":54,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')"],"metadata":{"id":"27ZsFZKAstah","executionInfo":{"status":"ok","timestamp":1754842811252,"user_tz":-330,"elapsed":1393,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n","docs = loader.load()"],"metadata":{"id":"_c9IVGVtsypX","executionInfo":{"status":"ok","timestamp":1754842811903,"user_tz":-330,"elapsed":645,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-_YCtM2s2cF","executionInfo":{"status":"ok","timestamp":1754842811940,"user_tz":-330,"elapsed":33,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"372b9a07-08e9-489d-f802-78dde6472fb4"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nGet started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability‚Äã\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\\n\\nGet started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals‚Äã\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering‚Äã\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\\n\\nGet started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n\")]"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")"],"metadata":{"id":"mpJseP42s5ej","executionInfo":{"status":"ok","timestamp":1754842811972,"user_tz":-330,"elapsed":27,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["embeddings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qja1eaoGtLVY","executionInfo":{"status":"ok","timestamp":1754842811992,"user_tz":-330,"elapsed":19,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"dae3d744-936e-4700-bebd-bc478944468e"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NVIDIAEmbeddings(base_url='https://integrate.api.nvidia.com/v1', model='nvidia/nv-embedqa-e5-v5', truncate='NONE', dimensions=None, max_batch_size=50)"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","documents = text_splitter.split_documents(docs)\n","vector = FAISS.from_documents(documents, embeddings)\n","retriever = vector.as_retriever()"],"metadata":{"id":"Rhs1Ec7AtQfi","executionInfo":{"status":"ok","timestamp":1754842812817,"user_tz":-330,"elapsed":817,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["retriever"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQ_PRvalyONK","executionInfo":{"status":"ok","timestamp":1754842858513,"user_tz":-330,"elapsed":54,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"6cf36cd7-ed23-4c86-dd45-591b3155b753"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VectorStoreRetriever(tags=['FAISS', 'NVIDIAEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7b00abbfaa90>, search_kwargs={})"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["model = ChatNVIDIA(model=\"openai/gpt-oss-20b\")"],"metadata":{"id":"wacRaDuNyQob","executionInfo":{"status":"ok","timestamp":1754842872254,"user_tz":-330,"elapsed":25,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb_UlgEVyT3l","executionInfo":{"status":"ok","timestamp":1754842883015,"user_tz":-330,"elapsed":13,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"6b100757-5921-4570-bd3e-02ffc5cf9bbc"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='openai/gpt-oss-20b')"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["model.invoke(\"tell me more about you?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XL6XHtbRyWq2","executionInfo":{"status":"ok","timestamp":1754842902801,"user_tz":-330,"elapsed":5907,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"b4508da4-b294-4ccd-fb75-ada75c4d2b91"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='I’m ChatGPT, a large‑language‑model (LLM) built by OpenAI. Here’s a quick rundown of what that means and what I can do for you:\\n\\n| What I am | How I work | What I can do | What I can’t do |\\n|-----------|------------|---------------|-----------------|\\n| **AI language model** | Trained on a diverse mix of books, websites, code, and other text up to **June\\u202f2024**. | • Answer questions, explain concepts, or provide background info.<br>• Help draft emails, essays, poems, or code snippets.<br>• Summarize articles or documents.<br>• Offer study or research assistance.<br>• Simulate conversation, role‑play, or brainstorming. | • I don’t have personal experiences or feelings.<br>• I can’t browse the web or fetch real‑time data.<br>• I can’t store or recall personal data beyond the current session. |\\n| **No consciousness** | I generate responses by predicting the next word in a sequence, based on patterns learned during training. | • I can adapt my tone and style to match your preferences (formal, casual, technical, etc.). | • I don’t have opinions, beliefs, or a “self.” |\\n| **Safety‑first design** | Built with guardrails to avoid disallowed content (e.g., hate speech, self‑harm instructions). | • I can provide safety‑related guidance (mental‑health resources, first‑aid basics, etc.) in a supportive way. | • I’m not a substitute for professional medical, legal, or financial advice. |\\n| **Multilingual** | Supports many languages—English, Spanish, French, Chinese, and more. | • Translate text, explain cultural nuances, or practice a new language. | • My proficiency varies; I’m strongest in English. |\\n\\n### How I was trained\\n- **Dataset**: A broad, publicly available corpus (books, Wikipedia, code repositories, etc.) plus licensed data. The data were filtered for quality and safety.\\n- **Architecture**: GPT‑4 (or the latest model I’m based on) uses a transformer neural network with billions of parameters, enabling me to understand context and generate coherent, context‑aware responses.\\n- **Fine‑tuning**: Human reviewers help steer the model toward helpful, safe, and accurate behavior. This is why I can answer a wide range of questions while staying within policy limits.\\n\\n### What to keep in mind\\n- **No memory across sessions**: Each conversation starts fresh. I don’t remember prior chats unless you provide context again.\\n- **Probabilistic output**: I generate the most likely next token, not a “truth.” Double‑check critical facts, especially if you’re making decisions based on my answers.\\n- **Bias & errors**: I can reflect biases present in the training data. If something feels off, feel free to ask for clarification or a different angle.\\n\\n### How I can help you today\\n- **Learning & tutoring**: Need help with math, science, history, or a programming language? I can explain concepts, walk through problems, or generate practice questions.\\n- **Writing & editing**: Draft or polish a report, story, or email. I can suggest structure, tone, and style tweaks.\\n- **Creative brainstorming**', additional_kwargs={'reasoning_content': 'The user says: \"tell me more about you?\" They want more info about me. We should respond with a friendly explanation about ChatGPT: my capabilities, training data, limitations, etc. The user may want to know about my architecture, training, usage, etc. We should mention that I\\'m a large language model trained by OpenAI, knowledge cutoff 2024-06, can\\'t browse the web, etc. We can also talk about how I can help them. The user didn\\'t specify a particular context, so a general overview is fine. We should be careful to not reveal private or internal details. We can mention that I\\'m an AI language model, can\\'t have personal experiences, etc. Also mention that I\\'m not a human, no consciousness. Provide details about how I can assist, what I can do, limitations. Possibly mention that I can generate text, answer questions, help with writing, coding, etc. Also mention that I can be used for conversation, but I have no personal opinions. Also mention that I can handle many languages. Also mention that my knowledge is up to date until 2024-06. Also mention that I can simulate a conversation. Also mention that I can\\'t provide personal data about myself beyond that. Also mention that I can\\'t recall past interactions beyond the current session. That is all. Also mention that I can do many tasks. So we can produce a friendly answer.\\n\\nWe should avoid any disallowed content. It\\'s fine. We can mention that I don\\'t have personal experiences, but I can share info about how I was trained. So let\\'s produce a friendly, thorough answer.'}, response_metadata={'role': 'assistant', 'reasoning_content': 'The user says: \"tell me more about you?\" They want more info about me. We should respond with a friendly explanation about ChatGPT: my capabilities, training data, limitations, etc. The user may want to know about my architecture, training, usage, etc. We should mention that I\\'m a large language model trained by OpenAI, knowledge cutoff 2024-06, can\\'t browse the web, etc. We can also talk about how I can help them. The user didn\\'t specify a particular context, so a general overview is fine. We should be careful to not reveal private or internal details. We can mention that I\\'m an AI language model, can\\'t have personal experiences, etc. Also mention that I\\'m not a human, no consciousness. Provide details about how I can assist, what I can do, limitations. Possibly mention that I can generate text, answer questions, help with writing, coding, etc. Also mention that I can be used for conversation, but I have no personal opinions. Also mention that I can handle many languages. Also mention that my knowledge is up to date until 2024-06. Also mention that I can simulate a conversation. Also mention that I can\\'t provide personal data about myself beyond that. Also mention that I can\\'t recall past interactions beyond the current session. That is all. Also mention that I can do many tasks. So we can produce a friendly answer.\\n\\nWe should avoid any disallowed content. It\\'s fine. We can mention that I don\\'t have personal experiences, but I can share info about how I was trained. So let\\'s produce a friendly, thorough answer.', 'content': 'I’m ChatGPT, a large‑language‑model (LLM) built by OpenAI. Here’s a quick rundown of what that means and what I can do for you:\\n\\n| What I am | How I work | What I can do | What I can’t do |\\n|-----------|------------|---------------|-----------------|\\n| **AI language model** | Trained on a diverse mix of books, websites, code, and other text up to **June\\u202f2024**. | • Answer questions, explain concepts, or provide background info.<br>• Help draft emails, essays, poems, or code snippets.<br>• Summarize articles or documents.<br>• Offer study or research assistance.<br>• Simulate conversation, role‑play, or brainstorming. | • I don’t have personal experiences or feelings.<br>• I can’t browse the web or fetch real‑time data.<br>• I can’t store or recall personal data beyond the current session. |\\n| **No consciousness** | I generate responses by predicting the next word in a sequence, based on patterns learned during training. | • I can adapt my tone and style to match your preferences (formal, casual, technical, etc.). | • I don’t have opinions, beliefs, or a “self.” |\\n| **Safety‑first design** | Built with guardrails to avoid disallowed content (e.g., hate speech, self‑harm instructions). | • I can provide safety‑related guidance (mental‑health resources, first‑aid basics, etc.) in a supportive way. | • I’m not a substitute for professional medical, legal, or financial advice. |\\n| **Multilingual** | Supports many languages—English, Spanish, French, Chinese, and more. | • Translate text, explain cultural nuances, or practice a new language. | • My proficiency varies; I’m strongest in English. |\\n\\n### How I was trained\\n- **Dataset**: A broad, publicly available corpus (books, Wikipedia, code repositories, etc.) plus licensed data. The data were filtered for quality and safety.\\n- **Architecture**: GPT‑4 (or the latest model I’m based on) uses a transformer neural network with billions of parameters, enabling me to understand context and generate coherent, context‑aware responses.\\n- **Fine‑tuning**: Human reviewers help steer the model toward helpful, safe, and accurate behavior. This is why I can answer a wide range of questions while staying within policy limits.\\n\\n### What to keep in mind\\n- **No memory across sessions**: Each conversation starts fresh. I don’t remember prior chats unless you provide context again.\\n- **Probabilistic output**: I generate the most likely next token, not a “truth.” Double‑check critical facts, especially if you’re making decisions based on my answers.\\n- **Bias & errors**: I can reflect biases present in the training data. If something feels off, feel free to ask for clarification or a different angle.\\n\\n### How I can help you today\\n- **Learning & tutoring**: Need help with math, science, history, or a programming language? I can explain concepts, walk through problems, or generate practice questions.\\n- **Writing & editing**: Draft or polish a report, story, or email. I can suggest structure, tone, and style tweaks.\\n- **Creative brainstorming**', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'token_usage': {'prompt_tokens': 73, 'total_tokens': 1097, 'completion_tokens': 1024, 'prompt_tokens_details': None}, 'finish_reason': 'stop', 'model_name': 'openai/gpt-oss-20b'}, id='run--c4473ca3-a756-4719-8847-fb5b12bd742e-0', usage_metadata={'input_tokens': 73, 'output_tokens': 1024, 'total_tokens': 1097}, role='assistant')"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["hyde_template = \"\"\"Even if you do not know the full answer, generate a one-paragraph hypothetical answer to the below question:\n","\n","{question}\"\"\""],"metadata":{"id":"FVFSSmbRydiE","executionInfo":{"status":"ok","timestamp":1754842929384,"user_tz":-330,"elapsed":44,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["hyde_prompt = ChatPromptTemplate.from_template(hyde_template)"],"metadata":{"id":"uSuikbJIyhyD","executionInfo":{"status":"ok","timestamp":1754842943050,"user_tz":-330,"elapsed":3,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["hyde_query_transformer = hyde_prompt | model | StrOutputParser()"],"metadata":{"id":"IAT_wgJOylJE","executionInfo":{"status":"ok","timestamp":1754842953636,"user_tz":-330,"elapsed":16,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["@chain\n","def hyde_retriever(question):\n","    hypothetical_document = hyde_query_transformer.invoke({\"question\": question})\n","    return retriever.invoke(hypothetical_document)\n",""],"metadata":{"id":"Qwr8shyhyp7P","executionInfo":{"status":"ok","timestamp":1754842966628,"user_tz":-330,"elapsed":40,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\""],"metadata":{"id":"WR6Ql3CSyrNv","executionInfo":{"status":"ok","timestamp":1754842978872,"user_tz":-330,"elapsed":6,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["prompt = ChatPromptTemplate.from_template(template)"],"metadata":{"id":"DMspIMuDyv4l","executionInfo":{"status":"ok","timestamp":1754842991207,"user_tz":-330,"elapsed":18,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["answer_chain = prompt | model | StrOutputParser()"],"metadata":{"id":"nRFenvm_yw6y","executionInfo":{"status":"ok","timestamp":1754843003734,"user_tz":-330,"elapsed":5,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["@chain\n","def final_chain(question):\n","    documents = hyde_retriever.invoke(question)\n","\n","    # FIXED: Check if any document has non-empty page_content\n","    if not documents or not any(getattr(doc, \"page_content\", \"\").strip() for doc in documents):\n","        raise ValueError(\"No valid documents retrieved for the question.\")\n","\n","    for s in answer_chain.stream({\"question\": question, \"context\": documents}):\n","        yield s"],"metadata":{"id":"nb2PhaKTy0BL","executionInfo":{"status":"ok","timestamp":1754843015477,"user_tz":-330,"elapsed":8,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["print(\"Retrieved documents:\", documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4mirtjYy23E","executionInfo":{"status":"ok","timestamp":1754843029526,"user_tz":-330,"elapsed":86,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"54935234-aa5d-4624-ef9f-15f1dac8b299"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieved documents: [Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='LangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"LangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability‚Äã\"), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Observability‚Äã\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals‚Äã\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Prompt Engineering‚Äã\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]\n"]}]},{"cell_type":"code","source":["for s in final_chain.stream(\"how can langsmith help with testing\"):\n","    print(s, end=\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoRcyoNWy7LQ","executionInfo":{"status":"ok","timestamp":1754843392896,"user_tz":-330,"elapsed":9328,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"7865f610-456e-434a-f33f-7b8fa889d3f3"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["LangSmith helps with testing by giving you a full set of observability and evaluation tools:\n","\n","* **Tracing & dashboards** – Add tracing to your app and create dashboards that show key metrics such as RPS, error rates, and costs.  \n","* **Evals** – Use LangSmith’s evaluation framework to build and run high‑quality evaluation datasets and metrics. It lets you score your application’s performance, run tests on production traffic, and gather human feedback on the data.  \n","* **Prompt engineering** – Iterate on prompts with automatic version control and collaboration features, making it easier to test and refine prompt logic.  \n","\n","Together, these features let you monitor, evaluate, and improve your LLM application throughout development and production."]}]},{"cell_type":"code","source":["import gradio as gr"],"metadata":{"id":"D6lFrgXOzHH4","executionInfo":{"status":"ok","timestamp":1754843091907,"user_tz":-330,"elapsed":6826,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["def ask_langsmith(question):\n","    return \"\".join(final_chain.stream(question))\n","\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## 🧠 GPT-OSS RAG\")\n","\n","    with gr.Row():\n","        inp = gr.Textbox(placeholder=\"Ask your question here...\", label=\"Your Question\")\n","    out = gr.Textbox(label=\"Answer\", lines=10)\n","\n","    btn = gr.Button(\"Submit\")\n","    btn.click(fn=ask_langsmith, inputs=inp, outputs=out)"],"metadata":{"id":"bTPm-1cZzMOJ","executionInfo":{"status":"ok","timestamp":1754843110726,"user_tz":-330,"elapsed":938,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["demo.launch(share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"id":"bYr_PZ0AzOJM","executionInfo":{"status":"ok","timestamp":1754843125637,"user_tz":-330,"elapsed":1638,"user":{"displayName":"Lakshman Mandapati","userId":"08692034506680306010"}},"outputId":"cec5f40a-4834-40f4-9509-dbbbf221394a"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://1e9251b4f8bd4953ea.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://1e9251b4f8bd4953ea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":81}]}]}